{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f344b66f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var nb = IPython.notebook;\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var command = \"NOTEBOOK_FULL_PATH = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
       "kernel.execute(command);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var nb = IPython.notebook;\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var command = \"NOTEBOOK_FULL_PATH = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3588cb9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory has been changed to /workspace/HtmlSecReg\n"
     ]
    }
   ],
   "source": [
    "# The working notebook file path is stored in NOTEBOOK_FULL_PATH\n",
    "# Change working directory to project root which is parent of NOTEBOOK_FULL_PATH\n",
    "# Typing Shift + Enter on obove cell to help proper javascript execution\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from jupyter_core.paths import jupyter_config_dir\n",
    "from traitlets.config import Config\n",
    "\n",
    "def get_config():\n",
    "  return Config()\n",
    "\n",
    "try:\n",
    "  config_file = os.path.join(jupyter_config_dir(), 'jupyter_notebook_config.py')\n",
    "  config_content = open(config_file).read()\n",
    "  exec(config_content)\n",
    "  root_dir = c['ContentsManager']['root_dir']\n",
    "  project_root = Path(root_dir + NOTEBOOK_FULL_PATH).parent\n",
    "  os.chdir(project_root)\n",
    "  print(f'Working directory has been changed to {project_root}')\n",
    "except:\n",
    "  raise Exception('Could not automatically change working directory.')\n",
    "\n",
    "# os.chdir('path/to/project/root/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7b2ad1a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad, Nadam\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "\n",
    "from article_body_recognizer.system_specs import char_emb_training_specs\n",
    "from article_body_recognizer.char_dict import vocabularies as vocab\n",
    "from article_body_recognizer.ANNs import charemb_comparator\n",
    "from article_body_recognizer.ANNs import charemb_network\n",
    "from article_body_recognizer import charemb_dataset\n",
    "from article_body_recognizer.training_utils import charrnn_encode_sequence\n",
    "from article_body_recognizer import charemb_training\n",
    "\n",
    "importlib.reload(charemb_dataset)\n",
    "importlib.reload(charemb_training)\n",
    "\n",
    "from article_body_recognizer import charemb_data_generator\n",
    "importlib.reload(charemb_data_generator)\n",
    "from article_body_recognizer.charemb_data_generator import DataBufferThread\n",
    "\n",
    "create_generator = charemb_dataset.create_generator\n",
    "start_buffer_data_thread = charemb_dataset.start_buffer_data_thread\n",
    "Tester = charemb_training.Tester\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddbe253e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "  'emb_trainable': True,\n",
    "  'pretrained_emb_vers': None, # str or None\n",
    "  'new_emb_vers': 'v5x10u03',\n",
    "  'new_trainer_version' : 'trainer_v5x10u03_re04_tictoc',\n",
    "  'lean_dataset': True,\n",
    "  'embedding_model_class': charemb_network.CharEmbeddingV5,\n",
    "  'comparison_norm_trainable': False,\n",
    "  'max_length': char_emb_training_specs['MAX_LENGTH'],\n",
    "  'min_length': char_emb_training_specs['MIN_LENGTH'],\n",
    "  'num_classes': char_emb_training_specs['NUM_CLASSES'],\n",
    "  # 'close_masking_ratio': 0.15,    # Many words are suitable to describe this idea:\n",
    "  # 'neutral_masking_ratio': 0.35,  # It's fundamentally flaw, naive, bad because it reveal obviously to model to easily predict which pair is simimlar and which one disimilar\n",
    "  'masking_ratio': 0.15,\n",
    "  'close_distance_scale': 1.0,\n",
    "  'neutral_distance_scale': -0.55,\n",
    "  'learning_rate': 1e-4,\n",
    "  'optimizer': RMSprop,\n",
    "  'batch_size': 4096,\n",
    "  'epochs': 3,\n",
    "  # 'buffer_size': 32,\n",
    "  'buffer_size': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b389a50",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training_sample_quantity 927000\n",
      "[INFO] training steps_per_epoch:  227\n",
      "[INFO] validating_sample_quantity 69948\n",
      "[INFO] validation_steps:  18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_training_steps(_cfg, _dataset):\n",
    "  sample_quantity = len(_dataset) * 3 if _cfg['lean_dataset'] else len(_dataset) * 4\n",
    "  steps_per_epoch = math.ceil(sample_quantity/_cfg['batch_size'])\n",
    "\n",
    "  return steps_per_epoch, sample_quantity\n",
    "\n",
    "def calculate_validating_steps(_cfg, _dataset):\n",
    "  sample_quantity = len(_dataset) * 2 if _cfg['lean_dataset'] else len(_dataset) * 4\n",
    "  steps_per_epoch = math.ceil(sample_quantity/_cfg['batch_size'])\n",
    "\n",
    "  return steps_per_epoch, sample_quantity\n",
    "\n",
    "validating_dataset = charemb_dataset.load_data('article_body_recognizer/charemb-dataset/validating', cfg)\n",
    "training1_dataset = charemb_dataset.load_data('article_body_recognizer/charemb-dataset/training1', cfg)\n",
    "training2_dataset = charemb_dataset.load_data('article_body_recognizer/charemb-dataset/training2', cfg)\n",
    "\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "BUFFER_SIZE = cfg['buffer_size']\n",
    "_lean_dataset = cfg['lean_dataset']\n",
    "\n",
    "steps_per_epoch, training_sample_quantity = calculate_training_steps(_cfg=cfg, _dataset=training1_dataset)\n",
    "print('[INFO] training_sample_quantity', training_sample_quantity)\n",
    "print('[INFO] training steps_per_epoch: ', steps_per_epoch)\n",
    "\n",
    "validation_steps, validating_sample_quantity = calculate_validating_steps(_cfg=cfg, _dataset=validating_dataset)\n",
    "print('[INFO] validating_sample_quantity', validating_sample_quantity)\n",
    "print('[INFO] validation_steps: ', validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8eee1c40",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def do_training():\n",
    "  configs = cfg.copy()\n",
    "  EPOCHS = configs['epochs']\n",
    "  BATCH_SIZE = configs['batch_size']\n",
    "\n",
    "  pretrained_model = None\n",
    "\n",
    "  _validating_dataset = validating_dataset\n",
    "\n",
    "  tmp_weight_filepath = \"article_body_recognizer/tmp/model_weights.h5\"\n",
    "  if Path(tmp_weight_filepath).exists():\n",
    "    os.remove(tmp_weight_filepath)\n",
    "\n",
    "  for i in range(EPOCHS):\n",
    "    print('[TRAINING] Grand Epoch: ', i)\n",
    "\n",
    "    if i % 2 == 0:\n",
    "      _training_dataset = training1_dataset\n",
    "      configs['emb_trainable'] = True\n",
    "      _epochs = 1\n",
    "    else:\n",
    "      _training_dataset = training2_dataset\n",
    "      configs['emb_trainable'] = False\n",
    "      _epochs = 3\n",
    "\n",
    "    steps_per_epoch, training_sample_quantity = calculate_training_steps(_cfg=configs, _dataset=_training_dataset)\n",
    "    print('[INFO] training_sample_quantity', training_sample_quantity)\n",
    "    print('[INFO] training steps_per_epoch: ', steps_per_epoch)\n",
    "\n",
    "    validation_steps, validating_sample_quantity = calculate_validating_steps(_cfg=configs, _dataset=validating_dataset)\n",
    "    print('[INFO] validating_sample_quantity', validating_sample_quantity)\n",
    "    print('[INFO] validation_steps: ', validation_steps)\n",
    "\n",
    "    split_type = 'training'\n",
    "    training_generator = DataBufferThread(\n",
    "      name='training',\n",
    "      dataset=_training_dataset,\n",
    "      model=pretrained_model,\n",
    "      split_type=split_type,\n",
    "      **configs,\n",
    "    )\n",
    "\n",
    "    split_type = 'validating'\n",
    "    validating_generator = DataBufferThread(\n",
    "      name='validating',\n",
    "      dataset=_validating_dataset,\n",
    "      model=pretrained_model,\n",
    "      split_type=split_type,\n",
    "      **configs,\n",
    "    )\n",
    "\n",
    "    model = charemb_comparator.CharembComparatorV1(configs)\n",
    "    if Path(tmp_weight_filepath).exists():\n",
    "      model.load_weights(tmp_weight_filepath)\n",
    "\n",
    "    model.fit(\n",
    "        training_generator.iter(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=_epochs,\n",
    "        validation_data=validating_generator.iter(),\n",
    "        validation_batch_size=BATCH_SIZE,\n",
    "        validation_steps=validation_steps,\n",
    "        shuffle='batch',\n",
    "    )\n",
    "\n",
    "    del pretrained_model\n",
    "    pretrained_model = model\n",
    "    pretrained_model.trainable = False\n",
    "    pretrained_model.save_weights(tmp_weight_filepath, overwrite=True)\n",
    "\n",
    "    training_generator.join()\n",
    "    validating_generator.join()\n",
    "\n",
    "    tester = Tester(dataset=_validating_dataset, model=pretrained_model, cfg=_scfg)\n",
    "    losses = tester.test_symmetric_distance()\n",
    "    print('Divergence of 2 distances of pair of samples: ', losses)\n",
    "    pred1_distance_means, pred2_distance_means = tester.test_distance_mean()\n",
    "    print('(pred1: input_1 vs input_2) Distance of 2 different samples: ', pred1_distance_means)\n",
    "    print('(pred2: input_2 vs input_1) Distance of 2 different samples: ', pred2_distance_means)\n",
    "\n",
    "  return pretrained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f284ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING] Grand Epoch:  0\n",
      "[INFO] training_sample_quantity 927000\n",
      "[INFO] training steps_per_epoch:  227\n",
      "[INFO] validating_sample_quantity 69948\n",
      "[INFO] validation_steps:  18\n",
      "[INFO] Primary deque maxlen:  6\n",
      "[INFO] Secondary deque maxlen:  3\n",
      "[INFO] Primary deque maxlen:  6\n",
      "[INFO] Secondary deque maxlen:  3\n",
      "[INFO] Training Char Embedding\n",
      "[CFG] emb_trainable:  True\n",
      "[CFG] comparison_norm_trainable:  False\n",
      "[CFG] optimizer :  <class 'keras.optimizer_v2.rmsprop.RMSprop'>\n",
      "[CFG] learning_rate :  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 17:05:53.371146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.400969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.401189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.401842: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-23 17:05:53.402313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.402483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.402637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.790309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.790505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.790662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-23 17:05:53.790811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6542 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2022-10-23 17:06:02.582722: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8500\n",
      "2022-10-23 17:06:03.051424: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-23 17:06:03.051685: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-23 17:06:03.051702: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-10-23 17:06:03.052013: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-23 17:06:03.052054: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 98s 399ms/step - loss: 2.3168 - distance_1_loss: 0.5289 - distance_2_loss: 0.5314 - distance_3_loss: 0.5499 - val_loss: 0.6777 - val_distance_1_loss: 8.9362e-04 - val_distance_2_loss: 0.0013 - val_distance_3_loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "char_model = do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448fc4c1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "trainable = cfg['emb_trainable']\n",
    "new_emb_vers = cfg['new_emb_vers']\n",
    "if trainable and new_emb_vers:\n",
    "  char_embedding_layer_weights = char_model.get_layer('char_embedding').get_weights()\n",
    "  with open(f'article_body_recognizer/pretrained_embedding/{new_emb_vers}.pickle', 'wb') as f:\n",
    "      pickle.dump(char_embedding_layer_weights, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f47ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "new_trainer_version = cfg['new_trainer_version']\n",
    "char_model.save_weights(f\"article_body_recognizer/pretrained_embedding/trainers/{new_trainer_version}.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d67a0b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def inspect_data(batch1, index):\n",
    "  texts = batch1[0]\n",
    "  labels = batch1[1]\n",
    "  texts_1 = texts['input_1'][index]\n",
    "  print('text 1 input shape: ', texts['input_1'].shape)\n",
    "  print('text 1 input: ', texts_1.tolist())\n",
    "  texts_2 = texts['input_2'][index]\n",
    "  print('text 2 input shape: ', texts['input_2'].shape)\n",
    "  print('text 2 input: ', texts_2.tolist())\n",
    "  label_1 = labels['distance_1'][index]\n",
    "  print('label 1 shape: ', labels['distance_1'].shape)\n",
    "  print('label 1: ', label_1)\n",
    "\n",
    "# training_generator, validating_generator = do_training( _cfg=cfg, _steps_per_epoch=steps_per_epoch, _validation_steps=validation_steps, _debug_generator=True)\n",
    "\n",
    "# Test looping\n",
    "# for b in validating_generator:\n",
    "  # time.sleep(1)\n",
    "  # pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe6ee8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# batch1 = next(training_generator)\n",
    "# index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf77ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# raise Exception('WIP.')\n",
    "# inspect_data(batch1, index)\n",
    "# index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0135f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "max_length = cfg['max_length']\n",
    "raw_data = [\n",
    "    {\n",
    "    'input_1': '<p>The exhibition will follow several high-profile fashion exhibitions for the VA, including <a>Balenciaga: \\\n",
    "                Shaping Fashion</a>, <a>Mary Quant</a> and the record-breaking <a>Christian Dior: Designer of Dreams</a>.</p>',\n",
    "    'input_2': '<p>The exhibition will follow several high-profile fashion exhibitions for the VA, including <a>Balenciaga: \\\n",
    "                Shaping Fashion</a>, <a>Mary Quant</a> and the record-breaking <a>Christian Dior: Designer of Dreams</a>.</p>',\n",
    "    },\n",
    "    {\n",
    "      'input_1': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the \\\n",
    "                perspectives of symmetry and invariance. </p>',\n",
    "      'input_2': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the \\\n",
    "                perspectives of symmetry and invariance. </p>',\n",
    "    },\n",
    "    {\n",
    "    'input_1': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the perspectives \\\n",
    "                of symmetry and invariance. </p>',\n",
    "    'input_2': '<p>The exhibition will follow several high-profile fashion exhibitions for the VA, including <a>Balenciaga: Shaping Fashion</a>, \\\n",
    "                <a>Mary Quant</a> and the record-breaking <a>Christian Dior: Designer of Dreams</a>.</p>',\n",
    "    },\n",
    "    {\n",
    "    'input_1': '<p>The exhibition will follow several high-profile fashion exhibitions for the VA, including <a>Balenciaga: Shaping Fashion</a>, \\\n",
    "                <a>Mary Quant</a> and the record-breaking <a>Christian Dior: Designer of Dreams</a>.</p>',\n",
    "    'input_2': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the perspectives of symmetry and invariance. </p>',\n",
    "    },\n",
    "    {\n",
    "      'input_1': '<div>Advertisement</div>',\n",
    "      'input_2': '<div>Advertisement</div>',\n",
    "    },\n",
    "    {\n",
    "      'input_1': '<div>Advertisement</div>',\n",
    "      'input_2': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the perspectives of symmetry and invariance. <p>',\n",
    "    },\n",
    "    {\n",
    "      'input_1': '<p>Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the perspectives of symmetry \\\n",
    "                and invariance. <p>',\n",
    "      'input_2': '<div>Advertisement</div>',\n",
    "    },\n",
    "]\n",
    "\n",
    "def transform_data(raw):\n",
    "  _data = {\n",
    "    'input_1': [],\n",
    "    'input_2': [],\n",
    "  }\n",
    "\n",
    "  for row in raw:\n",
    "    _data['input_1'].append(charrnn_encode_sequence(row['input_1'], vocab, max_length)[0])\n",
    "    _data['input_2'].append(charrnn_encode_sequence(row['input_2'], vocab, max_length)[0])\n",
    "\n",
    "  _data['input_1'] = np.array(_data['input_1'])\n",
    "  _data['input_2'] = np.array(_data['input_2'])\n",
    "\n",
    "  return _data\n",
    "\n",
    "samples = transform_data(raw_data)\n",
    "preds = char_model.predict_on_batch(x=samples)\n",
    "\n",
    "ind = 0\n",
    "output_1 = preds[0]\n",
    "output_2 = preds[1]\n",
    "output_3 = preds[2]\n",
    "for row in output_1:\n",
    "    print(output_1[ind],\"\\t\", output_2[ind],\"\\t\", output_3[ind])\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Tester(dataset=validating_dataset, model=char_model, cfg=cfg)\n",
    "\n",
    "losses = tester.test_symmetric_distance()\n",
    "print('Divergence of 2 distances of pair of samples: ', losses)\n",
    "\n",
    "pred1_distance_means, pred2_distance_means = tester.test_distance_mean()\n",
    "print('(pred1: input_1 vs input_2) Distance of 2 different samples: ', pred1_distance_means)\n",
    "print('(pred2: input_2 vs input_1) Distance of 2 different samples: ', pred2_distance_means)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
